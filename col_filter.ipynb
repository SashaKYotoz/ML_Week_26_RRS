{
 "cells": [
  {
   "cell_type": "code",
   "id": "be180762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:46:02.902832500Z",
     "start_time": "2026-01-27T17:45:31.272123Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Dataset (Optimized for Speed) ---\n",
    "class CoffeeDataset(Dataset):\n",
    "    def __init__(self, interactions, recipes, users):\n",
    "        # Create fast lookup dictionaries\n",
    "        self.user_map = users.set_index('user_id')[['taste_pref_bitterness', 'taste_pref_sweetness', 'taste_pref_acidity', 'taste_pref_body']].T.to_dict('list')\n",
    "        self.recipe_map = recipes.set_index('recipe_id')[['taste_bitterness', 'taste_sweetness', 'taste_acidity', 'taste_body']].T.to_dict('list')\n",
    "        \n",
    "        # Filter valid interactions\n",
    "        valid_interactions = []\n",
    "        for _, row in interactions.iterrows():\n",
    "            if row['user_id'] in self.user_map and row['recipe_id'] in self.recipe_map:\n",
    "                valid_interactions.append(row)\n",
    "        \n",
    "        self.data = pd.DataFrame(valid_interactions)\n",
    "        self.u_ids = self.data['user_id'].values\n",
    "        self.r_ids = self.data['recipe_id'].values\n",
    "        \n",
    "        # --- CRITICAL CHANGE: BINARY TARGETS ---\n",
    "        # We teach the model: 1 if rating >= 4 (Good), 0 otherwise.\n",
    "        # This makes ranking much easier to learn.\n",
    "        self.targets = (self.data['rating'].values >= 4).astype(np.float32)\n",
    "        \n",
    "        # We keep original ratings for NDCG calculation\n",
    "        self.raw_ratings = self.data['rating'].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self): return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u_feat = np.array(self.user_map[self.u_ids[idx]], dtype=np.float32)\n",
    "        r_feat = np.array(self.recipe_map[self.r_ids[idx]], dtype=np.float32)\n",
    "        return u_feat, r_feat, self.targets[idx], self.raw_ratings[idx]\n",
    "\n",
    "# --- 2. The Concatenation Model (MLP) ---\n",
    "# This architecture learns relationships BETTER than dot products\n",
    "class ConcatModel(nn.Module):\n",
    "    def __init__(self, input_dim=8): # 4 user feats + 4 recipe feats\n",
    "        super(ConcatModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2), # Helps prevent overfitting\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            \n",
    "            nn.Linear(32, 1) # Output one score (logit)\n",
    "        )\n",
    "\n",
    "    def forward(self, user_features, item_features):\n",
    "        # Combine user and item features into one vector\n",
    "        combined = torch.cat([user_features, item_features], dim=1)\n",
    "        return self.layers(combined).squeeze()\n",
    "\n",
    "# --- 3. NDCG Evaluation ---\n",
    "def evaluate_ndcg(model, val_df, recipes, users, k=5):\n",
    "    model.eval()\n",
    "    user_ndcgs = []\n",
    "    \n",
    "    # Fast Map Setup\n",
    "    u_map = users.set_index('user_id')[['taste_pref_bitterness', 'taste_pref_sweetness', 'taste_pref_acidity', 'taste_pref_body']].T.to_dict('list')\n",
    "    r_map = recipes.set_index('recipe_id')[['taste_bitterness', 'taste_sweetness', 'taste_acidity', 'taste_body']].T.to_dict('list')\n",
    "    \n",
    "    # Filter valid data\n",
    "    valid_val_df = val_df[val_df['user_id'].isin(u_map.keys()) & val_df['recipe_id'].isin(r_map.keys())]\n",
    "    grouped = valid_val_df.groupby('user_id')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user_id, group in grouped:\n",
    "            if len(group) < 2: continue\n",
    "            \n",
    "            # Prepare Batch\n",
    "            u_feat = torch.tensor([u_map[user_id]] * len(group), dtype=torch.float32)\n",
    "            r_feat = torch.tensor([r_map[r] for r in group['recipe_id'].values], dtype=torch.float32)\n",
    "            \n",
    "            # Ground Truth (Original 1-5 Ratings)\n",
    "            true_ratings = torch.tensor(group['rating'].values, dtype=torch.float32)\n",
    "            \n",
    "            # Get Scores (Logits)\n",
    "            preds = model(u_feat, r_feat) # Higher logit = higher probability of being \"Good\"\n",
    "            \n",
    "            # Sort by Model's Score\n",
    "            _, indices = torch.sort(preds, descending=True)\n",
    "            relevance_at_k = true_ratings[indices[:k]]\n",
    "            \n",
    "            # Ideal Sort\n",
    "            ideal_relevance, _ = torch.sort(true_ratings, descending=True)\n",
    "            ideal_relevance = ideal_relevance[:k]\n",
    "            \n",
    "            # Calc NDCG\n",
    "            discounts = torch.log2(torch.arange(2, len(relevance_at_k) + 2).float())\n",
    "            dcg = torch.sum(relevance_at_k / discounts)\n",
    "            idcg = torch.sum(ideal_relevance / discounts)\n",
    "            \n",
    "            ndcg = (dcg / idcg) if idcg > 0 else torch.tensor(0.0)\n",
    "            user_ndcgs.append(ndcg.item())\n",
    "            \n",
    "    return np.mean(user_ndcgs) if user_ndcgs else 0.0\n",
    "\n",
    "# --- 4. Main Training Loop ---\n",
    "if __name__ == '__main__':\n",
    "    # Load Data\n",
    "    try:\n",
    "        users_df = pd.read_csv('student_data/users.csv').fillna(0)\n",
    "        recipes_df = pd.read_csv('student_data/recipes.csv').fillna(0)\n",
    "        interactions_df = pd.read_csv('student_data/interactions_train.csv').fillna(2.5)\n",
    "        val_csv = pd.read_csv('student_data/interactions_val.csv').fillna(2.5) \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Split Train\n",
    "    train_df, internal_val = train_test_split(interactions_df, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Setup\n",
    "    train_dataset = CoffeeDataset(train_df, recipes_df, users_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # Initialize MLP Model\n",
    "    model = ConcatModel(input_dim=8) # 4 user + 4 recipe features\n",
    "    \n",
    "    # BCEWithLogitsLoss is standard for Binary Classification\n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) # Added weight_decay\n",
    "    \n",
    "    print(f\"Training MLP on {len(train_df)} samples...\")\n",
    "\n",
    "    for epoch in range(10): # Increased epochs\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for u_feat, r_feat, target, _ in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(u_feat, r_feat)\n",
    "            loss = criterion(logits, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Eval\n",
    "        if (epoch+1) % 2 == 0:\n",
    "            val_ndcg = evaluate_ndcg(model, internal_val, recipes_df, users_df)\n",
    "            print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Train-Val NDCG: {val_ndcg:.4f}\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    # FINAL TEST ON YOUR SEPARATE VALIDATION FILE\n",
    "    final_ndcg = evaluate_ndcg(model, val_csv, recipes_df, users_df)\n",
    "    print(f\"FINAL NDCG (Provided Val Set): {final_ndcg:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP on 68305 samples...\n",
      "Epoch 2 | Loss: 0.4945 | Train-Val NDCG: 0.9067\n",
      "Epoch 4 | Loss: 0.4903 | Train-Val NDCG: 0.9086\n",
      "Epoch 6 | Loss: 0.4881 | Train-Val NDCG: 0.9089\n",
      "Epoch 8 | Loss: 0.4875 | Train-Val NDCG: 0.9092\n",
      "Epoch 10 | Loss: 0.4884 | Train-Val NDCG: 0.9084\n",
      "------------------------------\n",
      "FINAL NDCG (Provided Val Set): 0.8133\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b9c4d074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:46:03.081115500Z",
     "start_time": "2026-01-27T17:46:02.906852500Z"
    }
   },
   "source": [
    "val_cold_csv = pd.read_csv('student_data/interactions_val_cold.csv').fillna(2.5)\n",
    "\n",
    "final_ndcg = evaluate_ndcg(model, val_cold_csv, recipes_df, users_df)\n",
    "print(f\"FINAL NDCG (Provided Val Cold Set): {final_ndcg:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL NDCG (Provided Val Cold Set): 0.8243\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
